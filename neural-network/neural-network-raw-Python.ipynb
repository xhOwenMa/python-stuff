{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e880c534",
   "metadata": {},
   "source": [
    "# Building a Neural Network in raw Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec3041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868d3fa8",
   "metadata": {},
   "source": [
    "### python inherent way of visualizing neurons\n",
    "\n",
    "- this is not to be used (it is so difficult :d); kept here mainly to archive the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce3f402",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "# made-up input (same as the output from previous neurons or directly from the sensors)\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "# different neurons have different set of weights and bias\n",
    "weights = [\n",
    "    [0.2, 0.8, -0.5, 1.0],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "]\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "\n",
    "layer_outputs = [] # output of this layer\n",
    "for neuron_weights, neuron_bias in zip(weights, biases):\n",
    "    neuron_output = 0;\n",
    "    for n_input, weight in zip(inputs, neuron_weights):\n",
    "        neuron_output += n_input*weight\n",
    "    neuron_output += neuron_bias\n",
    "    layer_outputs.append(neuron_output)\n",
    "\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3ad04c",
   "metadata": {},
   "source": [
    "### dot-product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e5698c",
   "metadata": {},
   "source": [
    "- using numpy dot-product; this is what will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87c8d681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2\n",
    "\n",
    "# the order in the dot-product is important\n",
    "output = np.dot(weights, inputs) + bias \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57ea7701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8   1.21  2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "weights = [\n",
    "    [0.2, 0.8, -0.5, 1.0],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "]\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "# the order in the dot-product is important\n",
    "output = np.dot(weights, inputs) + biases\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375343d6",
   "metadata": {},
   "source": [
    "## Batch, Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129e9fb5",
   "metadata": {},
   "source": [
    "- batch: n-dimensional inputs\n",
    "    - parallel processing\n",
    "    - generalizations\n",
    "    \n",
    "common size: 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36ed0b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "inputs = [\n",
    "    [1, 2, 3, 2.5],\n",
    "    [2.0, 5.0, -1.0, 2.0],\n",
    "    [-1.5, 2.7, 3.3, -0.8]\n",
    "]\n",
    "weights = [\n",
    "    [0.2, 0.8, -0.5, 1.0],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "]\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "output = np.dot(inputs, np.array(weights).T) + biases\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b79c2d",
   "metadata": {},
   "source": [
    "- add another layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "338e89cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5031  -1.04185 -2.03875]\n",
      " [ 0.2434  -2.7332  -5.7633 ]\n",
      " [-0.99314  1.41254 -0.35655]]\n"
     ]
    }
   ],
   "source": [
    "weights2 = [\n",
    "    [0.1, -0.14, 0.5],\n",
    "    [-0.5, 0.12, -0.33],\n",
    "    [-0.44, 0.73, -0.13]\n",
    "]\n",
    "biases2 = [-1, 2, -0.5]\n",
    "\n",
    "layer1_output = np.dot(inputs, np.array(weights).T) + biases\n",
    "\n",
    "layer2_output = np.dot(layer1_output, np.array(weights2).T) + biases2\n",
    "print(layer2_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff2aaca",
   "metadata": {},
   "source": [
    "### Object (OOP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf3b8c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1's output: \n",
      " [[ 0.10758131  1.03983522  0.24462411  0.31821498  0.18851053]\n",
      " [-0.08349796  0.70846411  0.00293357  0.44701525  0.36360538]\n",
      " [-0.50763245  0.55688422  0.07987797 -0.34889573  0.04553042]]\n",
      "layer 2's output: \n",
      " [[ 0.148296   -0.08397602]\n",
      " [ 0.14100315 -0.01340469]\n",
      " [ 0.20124979 -0.07290616]]\n"
     ]
    }
   ],
   "source": [
    "# input: conventionally denoted as X\n",
    "X = [\n",
    "    [1, 2, 3, 2.5],\n",
    "    [2.0, 5.0, -1.0, 2.0],\n",
    "    [-1.5, 2.7, 3.3, -0.8]\n",
    "]\n",
    "\n",
    "np.random.seed(0) # to control the randomness\n",
    "\n",
    "# to visualize what the __init__ is constructing: dimension of n_inputs by n_neurons \n",
    "#                                              --> this is to help not do the transpose in forward pass\n",
    "# print(0.10 * np.random.randn(4, 3)) \n",
    "\n",
    "# layer object:\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \"\"\"\n",
    "        n_inputs must be consistent to the input size\n",
    "        n_neurons is what you desire for this layer, any number (of neurons) that makes sense at this layer\n",
    "        \"\"\"\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "        \n",
    "layer1 = Layer_Dense(4, 5)\n",
    "layer2 = Layer_Dense(5, 2) # layer 2 is getting the output of layer 1 as its input\n",
    "\n",
    "layer1.forward(X)\n",
    "print(\"layer 1's output: \\n {}\".format(layer1.output))\n",
    "\n",
    "layer2.forward(layer1.output)\n",
    "print(\"layer 2's output: \\n {}\".format(layer2.output))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee73182",
   "metadata": {},
   "source": [
    "## Hidden Layer Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56449022",
   "metadata": {},
   "source": [
    "### Rectifier (ReLU):\n",
    "- need 2 layers (first control activation, second control deactivation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2d06311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "output = []\n",
    "\n",
    "# this is the ReLU function\n",
    "for i in inputs:\n",
    "    output.append(max(0, i))\n",
    "        \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695c97e5",
   "metadata": {},
   "source": [
    "### creating ReLU object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd2268d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9c72d7",
   "metadata": {},
   "source": [
    "#### generating a data set (spiral data function)\n",
    "- [github](https://gist.github.com/Sentdex/454cb20ec5acf0e76ee8ab8448e6266c)\n",
    "- this is from a Stanford [course](https://cs231n.github.io/neural-networks-case-study/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a08bd1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://cs231n.github.io/neural-networks-case-study/\n",
    "def spiral_data(points, classes):\n",
    "    X = np.zeros((points*classes, 2))\n",
    "    y = np.zeros(points*classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(points*class_number, points*(class_number+1))\n",
    "        r = np.linspace(0.0, 1, points)  # radius\n",
    "        t = np.linspace(class_number*4, (class_number+1)*4, points) + np.random.randn(points)*0.2\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = class_number\n",
    "    return X, y\n",
    "\n",
    "# to visualize:\n",
    "# X, y = spiral_data(100, 3)\n",
    "# plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8f968e",
   "metadata": {},
   "source": [
    "### ReLU Activation function application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "951c2d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 4.65504526e-04\n",
      "  4.56845892e-05]\n",
      " [0.00000000e+00 5.93467943e-05 0.00000000e+00 2.03573189e-04\n",
      "  6.10024276e-04]\n",
      " ...\n",
      " [1.13291515e-01 0.00000000e+00 0.00000000e+00 8.11079627e-02\n",
      "  0.00000000e+00]\n",
      " [1.34588354e-01 0.00000000e+00 3.09493973e-02 5.66337522e-02\n",
      "  0.00000000e+00]\n",
      " [1.07817915e-01 0.00000000e+00 0.00000000e+00 8.72561871e-02\n",
      "  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# layer object:\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \"\"\"\n",
    "        n_inputs must be consistent to the input size\n",
    "        n_neurons is what you desire for this layer, any number (of neurons) that makes sense at this layer\n",
    "        \"\"\"\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "np.random.seed(0)\n",
    "X, y = spiral_data(100, 3)\n",
    "\n",
    "layer1 = Layer_Dense(2,5)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "layer1.forward(X)\n",
    "# print(layer1.output)\n",
    "\n",
    "activation1.forward(layer1.output)\n",
    "print(activation1.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e88a4",
   "metadata": {},
   "source": [
    "### Softmax Activation (for output layer)\n",
    "- ReLU 'clips' every negative value to 0, which means it lost all its meaning\n",
    "- we want the output be some sort of a probability distribution so that we can learn something\n",
    "- exponential function $y = e^x$ helps to keep negative $x$'s implication in the output\n",
    "- then, normalize the exponentiated values into probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d71c63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values: \n",
      " [[1.21510418e+02 3.35348465e+00 1.08590627e+01]\n",
      " [7.33197354e+03 1.63654137e-01 1.22140276e+00]\n",
      " [4.09595540e+00 2.86051020e+00 1.02634095e+00]]\n",
      "normalized values: \n",
      " [[1.44741415e+01 3.99462138e-01 1.29351551e+00]\n",
      " [1.00575769e+03 2.24491271e-02 1.67544960e-01]\n",
      " [1.64694628e+00 1.15018504e+00 4.12682327e-01]]\n"
     ]
    }
   ],
   "source": [
    "layer_outputs = [\n",
    "    [4.8, 1.21, 2.385],\n",
    "    [8.9, -1.81, 0.2],\n",
    "    [1.41, 1.051, 0.026]\n",
    "]\n",
    "\n",
    "# exponentiate the values\n",
    "exp_values = np.exp(layer_outputs)\n",
    "print(\"exponentiated values: \\n {}\".format(exp_values))\n",
    "\n",
    "# normalizethe values\n",
    "# we want the following to apply to each row of data\n",
    "# norm_values = exp_values / np.sum(exp_values); achieved by the following code\n",
    "# axis = 1 helps to sum each row, keepdims helps to give the output of correct shape\n",
    "norm_values = exp_values / np.sum(layer_outputs, axis = 1, keepdims = True)\n",
    "\n",
    "print(f\"normalized values: \\n {norm_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968fb498",
   "metadata": {},
   "source": [
    "- exponentiation function grows rapidly, very easy to encounter overflow error\n",
    "- we subtract the input values by the maximum values of those inputs then do the exponentiation\n",
    "- this way the range of exponentiation can only goes to 1, all the other input values would be negative and thus exponentiation of them goes to some value between 0 and 1:\n",
    "    - for all $v \\in \\vec{V}$: $v - \\max{v_i}$; then take $y = e^{v_i}$; now it must be that $y \\in \\{0, 1\\}$ because all $v_i \\in \\{-\\infty, 0\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f28350",
   "metadata": {},
   "source": [
    "### Softmax object class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dbee34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
    "        # this is what is discussed above, preventing the overflow error\n",
    "        \n",
    "        probs = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "        self.output = probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6eb8f98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33331734 0.33331832 0.33336434]\n",
      " [0.3332888  0.33329153 0.33341967]\n",
      " [0.33325941 0.33326395 0.33347665]\n",
      " [0.33323311 0.33323926 0.33352763]]\n"
     ]
    }
   ],
   "source": [
    "# overall example so far:\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \"\"\"\n",
    "        n_inputs must be consistent to the input size\n",
    "        n_neurons is what you desire for this layer, any number (of neurons) that makes sense at this layer\n",
    "        \"\"\"\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
    "        # this is what is discussed above, preventing the overflow error\n",
    "        \n",
    "        probs = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "        self.output = probs\n",
    "\n",
    "        \n",
    "np.random.seed(0)\n",
    "X, y = spiral_data(100, 3)\n",
    "\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "print(activation2.output[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db980550",
   "metadata": {},
   "source": [
    "## Calculating Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e123282",
   "metadata": {},
   "source": [
    "- with Softmax, output is a probability distribution, therefore categorical, this leads to:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5434cff9",
   "metadata": {},
   "source": [
    "### Categorical Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1d8d33",
   "metadata": {},
   "source": [
    "$$L_i = -\\log(\\hat{y}_{i,k})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0447890a",
   "metadata": {},
   "source": [
    "- $L_i$: sample loss value\n",
    "- $\\hat{y}$: predicted value\n",
    "- $i$: i-th sample in a set\n",
    "- $k$: target label index, index of correct class probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58de1370",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e535487",
   "metadata": {},
   "source": [
    "- One-hot vector: size is of 'classes', and all $0$ except at the index of 'Label' (where the value is $1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4709578a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35667494 0.         0.        ]\n",
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "# example:\n",
    "\"\"\"\n",
    "Classes: 3\n",
    "Labels: 0\n",
    "One-hot vector: [1, 0, 0]\n",
    "Prediction: [0.7, 0.1, 0.2]\n",
    "\"\"\"\n",
    "\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "target_output = [1, 0, 0]\n",
    "\n",
    "loss = -(np.log(softmax_output) * target_output)\n",
    "print(loss)\n",
    "\n",
    "# observe the two vectors, note the above is the same as:\n",
    "loss2 = -(np.log(softmax_output[0]))\n",
    "print(loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b31ea",
   "metadata": {},
   "source": [
    "## Implement Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d334c95",
   "metadata": {},
   "source": [
    "- since we will be working with batches: we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a58c006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n",
      "[0.35667494 0.69314718 0.10536052]\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array(\n",
    "    [[0.7, 0.1, 0.2],\n",
    "     [0.1, 0.5, 0.4],\n",
    "     [0.02, 0.9, 0.08]]\n",
    ")\n",
    "class_targets = [0, 1, 1]\n",
    "\n",
    "print(softmax_outputs[[0,1,2], class_targets]) # the [0,1,2] are the index in the np.array; \n",
    "# then the class_targets refer to which element in that vector\n",
    "\n",
    "neg_log = -np.log(softmax_outputs[[0,1,2], class_targets])\n",
    "print(neg_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01134067",
   "metadata": {},
   "source": [
    "-  one subtlety: $-\\log(0) = \\infty$, this mess up when we want to take the mean\n",
    "- therefore we 'clip' the softmax_outputs by setting the range from (1e-7) to (1 - 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5ef2fc",
   "metadata": {},
   "source": [
    "### Loss Object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c831f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "    \n",
    "class Loss_CatCrossEntropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        # clip the values:\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1: # this means y_true is a simple vector\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2: # this means the y_true is in array of one-hot vectors\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis = 1)\n",
    "            \n",
    "        neg_log_likelihoods = -np.log(correct_confidences)\n",
    "        return neg_log_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "477938ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0984449588022467\n"
     ]
    }
   ],
   "source": [
    "# continue from above overall example:\n",
    "\n",
    "def spiral_data(points, classes):\n",
    "    X = np.zeros((points*classes, 2))\n",
    "    y = np.zeros(points*classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(points*class_number, points*(class_number+1))\n",
    "        r = np.linspace(0.0, 1, points)  # radius\n",
    "        t = np.linspace(class_number*4, (class_number+1)*4, points) + np.random.randn(points)*0.2\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = class_number\n",
    "    return X, y\n",
    "\n",
    "# to visualize:\n",
    "# X, y = spiral_data(100, 3)\n",
    "# plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \"\"\"\n",
    "        n_inputs must be consistent to the input size\n",
    "        n_neurons is what you desire for this layer, any number (of neurons) that makes sense at this layer\n",
    "        \"\"\"\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
    "        # this is what is discussed above, preventing the overflow error\n",
    "        \n",
    "        probs = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "        self.output = probs\n",
    "        \n",
    "\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "    \n",
    "class Loss_CatCrossEntropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        # clip the values:\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1: # this means y_true is a simple vector\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2: # this means the y_true is in array of one-hot vectors\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis = 1)\n",
    "            \n",
    "        neg_log_likelihoods = -np.log(correct_confidences)\n",
    "        return neg_log_likelihoods\n",
    "\n",
    "        \n",
    "np.random.seed(0)\n",
    "X, y = spiral_data(100, 3)\n",
    "\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# print(activation2.output[:5])\n",
    "\n",
    "loss_function = Loss_CatCrossEntropy()\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfd3baa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
